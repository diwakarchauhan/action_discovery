\documentclass[a4paper, 10pt, notitlepage]{report}

\usepackage{amsfonts} % if you want blackboard bold symbols e.g. for real numbers
\usepackage{amsmath}
\usepackage{graphicx} % if you want to include jpeg or pdf pictures
\usepackage{devanagari}
\usepackage{xcolor}
\usepackage{geometry}
\geometry{
    body={6.0in,9.5in},
    left= 1.0in,
    top = 0.5in}

\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=blue,
    urlcolor=black
}
\usepackage{color}
\definecolor{gray1}{RGB}{192,192,192}
\definecolor{gray}{RGB}{240,240,240}
%\title{Action Discovery in Psychological Videos and Hindi Verb Modelling} % change this
\author{Diwakar Chauhan} % change this
\date{\today} % change this

\begin{document}

\begin{center}
%\renewcommand{\rmdefault}{ptm}

{\color{blue} \fontfamily{rmdefault} \textsc{\LARGE IIT Kanpur}\\[1.5cm] }
\textsc{\Large B.Tech Project}\\[1cm]
\hrule height 2pt
\vspace*{10pt}
%\end{center}
{\huge  Action Discovery in Psychological Videos and Hindi Verb Modelling}\\[0.4cm] 
\hrule height 2pt 
\vspace*{10pt}
%\\[1cm]
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
{\color{blue} \href{http://home.iitk.ac.in/~diwakarc}{Diwakar Chauhan} }
\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
{\color{blue} \href{http://www.cse.iitk.ac.in/users/amit}{Prof. Amit Mukerjee } }% Supervisor name - remove the \href bracket to remove the link  
\end{flushright}
\end{minipage}\\[3cm]


{\large \today}\\[4cm] % Date
%%%%%%%%%% PRELIMINARY MATERIAL %%%%%%%%%%
%\maketitle
%\begin{center}
%Supervised by Dr. Amit Mukerjee % change this
\end{center}

\thispagestyle{empty}

\newpage
\section*{Acknowledgements} % this must be included in undergradate projects
I am very thankful to people who have helped me with this project directly or indirectly.
I am thankful to my project supervisor for guiding me through this project.Due to his sincere and timely guidance, I was able to do thie project.  \\
Many-many thanks to Dr. Uta Frith, Institute of Cognitive Neuroscience,University College London, and  her Ph.D student Sarah White for providing me with psychological videos which were the core of the project\textsl{â€¢}.\\%acknowledgement for utah Frith
I am thankful to Mr. Sushobhan Nayak for his work on language acquisition. This helped me a lot in the project. %Sushobhan Nayak\\
I am also thankful to Mr Prashant Jalan for doing syllable based noun discovery on the same video. Syllable based analysis helped to understand more about the co-relation of objects with utterances. He helped me in collecting the commentaries and transcribing to text.

\tableofcontents 

\newpage
%%%%%%%%%% MAIN TEXT STARTS HERE %%%%%%%%%%
\section*{Abstract}
In this project we are trying to discover nouns and verbs from visual sequences and corresponding narrations. We are trying to use similar process as the infants use. We don't use any prior knowledge about the visual sequence or language as do the infants in their early learning process.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%% SAMPLE CHAPTER %%%%%%%%%%
\chapter{Introduction}
\hrule height 2pt
\vspace*{10pt} 

\section{Human Learning Process :}
Human learning process involves learning the identity of objects(nouns) the relationship between the object(preposition) and the interaction of objects. An infant learns the identities of objects in first place than verbs. The reason behind this may be that verbs therefore activities, require more information. 
The infants have no prior knowledge about the environment. They see shapes, faces and moving objects in in front of them. At the same time they hear utterances of the people. The utterances spoken around the child are usually proper sentences. These contain diverse range of words. But there are some words which are specific to that particular object or action. Initially the kids don't know the word boundaries. So they try to map the phonetics of utterances with the visual sequences. Once they know the word boundaries, they map words to the visual sequences. So in both ways they filter out the words specific to visual sequences out of all the utterance and ignore the common or irrelevant words.

Naigles(1990)\cite{naigles} demonstrated that while learning meaning of verbs, infants use the syntactic information. She proved that given a illegal verbs in a sentence the children use the syntax to guess the meaning of the verb. \\

\hspace*{10pt} However difficult the learning process of infants may be, they do not analyse large amount of data in order to learn the meanings of verbs or recognize the activities. They learn from the competent speakers who know to relate the words to the event's objects in the environment.\cite{kerr-cohen-08_wubble-world-lang-acquisition} And the children can extend their learning from one event to another. 
\hspace*{10pt} Here we are trying to understand the leaning process of infant and based on that learning process, try to discover actions in psychological videos and map Hindi verbs to them.

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.5]{childLearn.png}
\end{tabular}
\label{tab:gt}
\caption{The schematic of learning process of child}
\end{figure}


\section{Actions} 
\hspace*{10pt} Actions can be based on the the variations in the shape and size change , poses\cite{pami-Ben-ArieWPR02}  or orientation of the object. For example a person walking can be identified by motion of his hands and legs relative to his body. Similar can be said about the actions involving multiple agents. But what if the action which either don't need any shape changes to be expressed or the agent is an abstract object. These type of actions are represented either by specific motion of single object or particular type of motions and interaction between the agents. In this project we analyse the second type of actions.
%%place samples from the video
\hspace*{10pt} Most of these animations contain only two agents which are triangles. The videos represent a particular transitive action.The videos,each of them represents certain action either complex or simple. By complex action we mean that the action constitutes smaller actions, e.g. the "Coxing" in the video consists of many instances of {\dn dhakkA denA} and {\dn ghumnA}. We take some of the videos and apply HMM to on the feature vectors extracted from the frames of video. We take frames in small consecutive groups. We create HMM for each of this groups and evaluate the mutual acceptance measure for each of the groups. Based on this measure, hierarchical clustering is created. This cluster tree is later cut at some point to produce certain number of clusters.\\
\hspace*{10pt} We label the video with actions occurring in video. Then we merge the most similar clusters using HMMs. On theses clusters, the associated commentaries are calculated. We calculate the relative frequency measure for words in a cluster. After that remove nouns and common words to get the verbs for the clusters.\\
\hspace*{10pt} At the same time commentary is taken for the video. This commentary is processed to remove less important words. Then we take the part of commentaries which are more relevant to the triangles using attention model. And then we calculate association measures for each of the remaining words. Based on this association measure, the nouns are identified.\\
Below is the schematic of the whole process used in the project :- 
\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.5]{schematic.png}
\end{tabular}
\label{tab:gt}
\caption{The schematic of the whole process}
\end{figure}

\chapter{Prelinguistic Image Schemata}
%\vspace*{5pt}
\hrule height 2pt
\vspace*{10pt}
Before mapping the language to visual sequences, we need to get the properties of the visual sequences.\\
\section{Agents in the visual sequences}
A sentence in a language consists of small units of words, phrases appearing as different parts of speech for that language. Theses different words are mapped to different properties of visual sequences. For example the nouns are used in context of the shapes occurring in the video. The prepositions are used for the relative positions of objects or similar things. The verbs are mapped to the action sequences. Action sequences are expressed by involvement of one or more agents.\\
\hspace*{10pt} In this project we use Frith-Happe \cite{fhanimation} animations. In these videos the objects are red triangle(also big triangle), blue triangle(also small triangle) and a rectangular box. The rectangle is static at some position in the video. The two triangles move to represent actions.\\

\begin{figure}[h]
\center
\begin{tabular}{ccc}
\includegraphics[scale=0.3]{sample65dance.png} & 
\includegraphics[scale=0.15]{sample174chase.png} &
\includegraphics[scale=0.3]{sample198coax.png}
\end{tabular}
\label{tab:gt}
\caption{Stills from animations (a) Dancing Video, (b) Chase Video and (c) Coaxing Video }
\end{figure}

\section{Extracting Objects: }
We selected $4$ of the videos namely Coaxing and Chase to use in our project. In these videos there were three objects, a red triangle, a smaller blue triangle and and a rectangular box. In the rest of the two videos the rectangular box is absent.\\
The frames are converted to gray scale and following steps are done :\\
\\ \hspace*{10pt} In the first step, we determine the two triangles in the video and track them by background subtraction. To do so one frame of the video is taken as the base frame $frame_0$ and for all the images we calculate the difference of frames :
	$$difference_i = frame_i - frame_0$$
Then we apply erosion and dilation to remove the noise in the image. At this step the triangles are not separated from each other but they are separated from rest of the static objects in the video. We use various image processing techniques to separate the triangles which are described below.
\hspace*{10pt} The triangles are made of lines segments. Therefore the most obvious way of determining a triangle is to determine it's edges. Hough transform is a very effective way of identifying the line segments in a data.\\

\subsection{Separation of the triangles}
\hspace*{10pt} In the video the sizes of both the triangles are significantly different. We take the benefit of f this difference. The line segments corresponding to the bigger triangle will have high intensity in the $(R, \Theta)$ space compared to that of the smaller triangle.\\
\hspace*{10pt} We take top three peaks in the $(R, \Theta)$ space and get the points corresponding to them in $(X,Y)$ space. These points form the bigger triangle. Once the line of a single triangle has been extracted, we can easily get its vertices by calculating the intersection points of the lines.\\
\hspace*{10pt} After separating the larger triangle, we apply same Hough Transform technique in the remaining points to get the edges of the smaller triangle and vertices subsequently.\\

\begin{figure}[h]
\center
\begin{tabular}{cc}
\includegraphics[scale=0.30]{0002l.png} & 
\includegraphics[scale=0.30]{0002s.png}
\end{tabular}
\label{tab:gt}
\caption{Two triangles as detected by Hough Transform}
\end{figure}

\subsection{Determination of the Rectangle }
Since the rectangle is static in the video. We can determine it in just one frame. After the triangles are identified and separated from the frame, we remove the white background and the black-red boundary. Then we are left with the points corresponding rectangle. On these points, applying hough transform, the four edges of the rectangle are determined. The four vertices of the rectangle are determined by intersection of the edges. And the vertices of the opening in the rectangle is determined by linear interpolation.\\
\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.30]{rectCoax.png}
\end{tabular}
\label{tab:gt}
\caption{The detected rectangle and its vertices in Coaxing Video}
\end{figure}


\section{Feature Extraction}
 Feature extraction is one of the most important parts of this project. The actions in a image sequence are represented bye the motion of the objects in the video. Therefore the motion of the objects in the video is highly dependent on the other object. To discover actions in the video, we need to get the features which are relevant to their interaction and their individual actions.\\
 \hspace*{10pt} Let the feature vector of a frame is $F = \{f_1 , f_2, \cdots, f_n\}$. Here are the features which we calculated and used different subsets of these in further works :- \\

 \subsection{Position of the Triangles : }
The interaction between the objects is highly dependent on the position of the objects. The individual position of the objects also infer their relative positions.\\
\hspace*{10pt} In object identification, we calculated the coordinates of the vertices of the triangles. From them we calculate the coordinates of the centroid of each of the triangle which is also assumed to be the position of the triangle.

\subsection{Orientation of the Triangles : }
The extent of interaction between the triangles is dependent on their relative orientation, e.g. if both the triangles are facing each other(exactly opposite orientation), then they are more likely to interact or express an action.\\
\hspace*{10pt} The orientation of the triangle is calculated by tracking the motion of the triangle. The vertex relatively aligned in the direction of the motion with respect to the centroid of the triangle is the head of the triangle. The made by the line joining the centroid with the head of the triangle is the orientation of the triangle.

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.30]{orientation.png}
\end{tabular}
\label{tab:gt}
\caption{The detected rectangle and its vertices in Coaxing Video}
\end{figure}

\subsection{Visibility of the triangle :}
In the videos where the rectangular box is there, the fact that whether the two triangles are able to see each other or not and if they can then to what extent. Therefore we chose visibility too as a feature vector in the triangle.\\
\hspace*{10pt} The visibility of two triangle is a non-negative number $v \in [0, 1]$. Here $0$ means that the two triangles are completely occluded with the rectangle and $1$ means that they are completely visible to each other and any value between $0$ and $1$ is measure of extent they are visible.\\
\hspace*{10pt} The visibility of the triangles is calculated as follows :- \\
\hspace*{20pt} $1-$ Draw the supporting tangents for the two triangles. Now part of the rectangle which falls between these two lines is determined. We take the projection of this part of rectangle in directions of perpendicular to the  supported tangents is calculated. The fraction of this projection with total distance between supporting tangents is the measure of the visibility of the triangles. The figure below explains feature :- \\

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.30]{visibility.png}
\end{tabular}
\label{tab:gt}
\caption{Visibility of triangles :  The brown line between the two supporting tangents is the distance between the lines and the green line is measure of occlusion and the ratio of lengths of green and brown lines is the measure of visibility}
\end{figure}

\subsection{Feature vector :}
The feature vector of a frame is a subset or derived of the previously calculated features. Let the centroid is represented by $C$ and orientation is represented by$\theta$ and the visibility between the triangles $t_1$ and $t_2$ is $v$. Then Following are some feature vector used :-\\

1- Ignore the visibility factor in both the triangles and take only the centroids and orientation of the triangles \\
\hspace*{20pt} $[C_{t_1}$ $C_{t_2}$ $\theta_1$ $\theta_2]$\\
2- Include the visibility factor with other features\\
\hspace*{20pt} $[C_{t_1}$ $C_{t_2}$ $\theta_1$ $\theta_2$ $v]$\\
3- Since $\theta$ is in $S_1$ topology, i.e it is periodic value with period $360$. It's values of $0$ and $360$ is overlapping. Therefore to remove this shortcoming, we can use the $sin$ and $cos$ of the $\theta$.\\
\hspace*{20pt} $[C_{t_1}$  $C_{t_2}$  $sin\theta_1$  $cos\theta_1$  $sin\theta_2$  $cos\theta_2]$\\

\section{Actions in the Visual Sequence :}
Actions in visual sequences are result of complicated motions of the objects in it. This complicated motion can be learnt by HMMs and appropriate feature vectors. We follow two approaches for learning the actions in the video. One is completely unsupervised hierarchical cluster based method and other is user labelled unsupervised method. These two methods differ on how we provide the input to the HMMs and how to use the created HMHs.\\

\colorbox{gray}{\Large \bf A. Completely Unsupervised Method} 

\subsection{Learning the HMM :}

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
We break the whole video into frame sequences each having $N$ frames and each of sequences overlap by $M$ frames. The choice of $N$ and $M$ is dependent on the approximate length of the action sequences and the time interval in which the action appears. For each of these sequence we learn HMM on that.

\end{flushleft}
\end{minipage}
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large

\center
\begin{tabular}{c}
\includegraphics[scale=0.5]{interval.png}
\end{tabular}
\label{tab:gt}

\end{flushright}
\end{minipage}\\[3cm]

\subsection{Hierarchical Clustering :}
Once we get the HMMs on all the image sequences, we calculate the distance between these HMMs. Here we take the Mutual Acceptance as the distance between to HMM. Mutual acceptance is defined as :-\\

$$dist(S_1, S_2) = \frac{|logP(S_2|\lambda _ 1)|}{N_1} +\frac{|logP(S_1|\lambda _ 2)|}{N_2} $$
Where : \\
\hspace*{20pt} $\lambda_1$ and $\lambda_2$ are HMMs trained on data $S_1$ and $S_2$ respectively.\\
\hspace*{20pt} The length of $S_1$ is $N_1$ and that of $S_2$ is $N_2$.\\

Based on this measure a hierarchical clustering is created. We used in-built function of Matlab for such clustering. A hierarchical clustering is recursively merging of clusters or points to form a single cluster in the end. Thus we get tree in which different clusters are merged at different levels based on the distance measure provided and a merging method. We used Mutual acceptance as calculated above as the distance measure and 'ward' method for merging clusters.

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.30]{dendoCoax.png}
\end{tabular}
\label{tab:gt}
\caption{The hierarchical clustering created for Coaxing video, top 30 nodes are shown}
\end{figure} 
 
 The leafs in the tree represent the individual points in the dataset while the internal nodes represent the clusters of points. The points merging near the leaves are more similar to points merging far from leaves. This hierarchical clustering helps us to classify the data into specific number of clusters. We just need to cut the tree at appropriate level for this.\\
 
 \colorbox{gray}{\Large \bf B. Used labelled Unsupervised Method}

\subsection{Labelling the actions :}
In the Coaxing video, there are 4-5 types of actions namely {\dn ghUma , .takkara mAranA, khela, la.da}. Here we label all the actions and their occurrence interval in the video manually. Multiple intervals can have same actions.

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.50]{userLabelActions.png}
\end{tabular}
\label{tab:gt}
\caption{User labelled actions in the Coaxing Video}
\end{figure} 

\subsection{Learning HMM on labelled actions and Merging labels:} 
In the next step we learn HMM on each of the labels. These HMMs are characteristics of the action described by that label. Now based on Mutual acceptance distance we merge the nearest HMMs. After merging we will be left with set of intervals each representing a single type of action.\\

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.30]{barplot.png}
\end{tabular}
\label{tab:gt}
\caption{HMM distances of first labelled action sequence with all other major action sequences}
\end{figure} 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Vision and Language Dataset }
\hrule height 2pt
\vspace*{10pt} 
In order to recognise the actions and corresponding language labels for the language, we need proper datasets of visual sequences as well as language. The visual dataset should be something which contains various type of actions clearly represented. And similarly language database for the visual sequences should be correctly aligned to the visual sequences.
\section{ Visual Database :} The video used in this project are psychological videos. These videos were made by Dr. Uta Frith. She is a professor at Institute of Cognitive neuroscience, University of London. These videos were initially created for the purpose of identification of Autism in children and their further evaluation. There are three classes in the dataset, each containing 4 videos : \\
\hspace*{10pt}{\bf 1- GD :} This class contains videos with goal directed actions.The videos in this class are chase,dance, fight, lead.\\ 
\hspace*{10pt}{\bf 2- TOM :} This class contains videos testing Theory of Mind. The videos in this category are coax, mock, seduce, surprise.\\
\hspace*{10pt}{\bf 3- Random :} In this class the videos represent random actions likely drift, billiard, star, tennis.

We recognised action clusters for chase and coax videos. The language modelling is done for the coaxing video due to its rich content of actions.
\subsection{Motion Data of objects in the video :}
In the videos we need to get the motion data of the objects in it. Motion data involves the frame intervals in which a particular object is moving and when it is not moving. Since in our videos only the triangles move, we need the motion data of these two triangles. This data is further used in recognition of nouns and verbs subsequently.

\section{Language Database : } Language database is created by manually taking the commentaries for the videos and then transcribing them into text. We took commentaries in two languages primarily for one video 'Coaxing'. $22$ commentaries in Hindi and $8$ commentaries in English.

\subsection{Collecting Commentaries :} Different people were asked to give commentary on the video. One subject could give at most on commentary in each of the language. Each subject was shown the video multiple times before giving the commentaries. The purpose of showing the video before commentary was to make the subject familiar with the environment. This would help him to speak simultaneously with the actions in the video at the time of giving the commentary.\\
\hspace*{10pt} Each subject was given certain instructions about the commentary. The instructions are as follows :-

{\it 
1 - Describe the objects in the video and their interactions.\\
2 - Do not involve yourself or any other external object as agent in the video.\\ 
3 - Do not metaphorize the sentences, explain them as they appear. \\
4 - Try to speak simultaneously \\
}
The subjects showed huge variety in their description of the video. Majority of the subjects considered the two triangles and the one side opened rectangular box. But some of subjects considered the rectangular shape enveloping all other objects as another object. Therefore their narration consisted of $4$ agents. Most of the subjects were stuck on their initial notion of the objects in the video, e.g. If a subject refers the larger triangle(also the red one) as {\dn ba.dA tribhuj} then he continues to refer it as this word only in the whole commentary and if he refers it as {\dn lAla tribhuj} then he does it in whole video. Very a few subjects mixed these words.  \\
After recording the commentaries, the audio files were embedded the the video. For each of the subject and language, a separate video was created with equal frame rate with the original animation video.\\

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.70]{comm.png}
\end{tabular}
\label{tab:gt}
\caption{Transcribed text for Hindi commentary}
\end{figure} 

\subsection{Transcribing to text :}
We did manual transcription of the commentaries. We followed a common procedure for transcribing commentaries of both the languages. The minimal unit of transcription was words. The words were separated from each other by spaces. The morphological variations of the words were kept as they were spoken. The small grammatical errors were corrected.\\
Now the utterances were divided into sentences or phrases. The basis of division was the length of sentences or phrases and the pauses made by the subject. If a sentence was larger than some predefined length and could be broken into two parts, we wrote it in two parts. Or if the subjects pauses for more than $5$ frames before uttering next words, we consider it in next line. The pauses of less than $5$ frames were bound to accuracy of measurement.\\

\begin{figure}[h]
\center
\begin{tabular}{c}
\includegraphics[scale=0.70]{commEng.png}
\end{tabular}
\label{tab:gt}
\caption{Transcribed text for one English commentary}
\end{figure} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Language Association } 
\hrule height 2pt
\vspace*{10pt} 

 Till now we are able to separate different objects in the video. Now we need to associate these objects with words. In the similar way by clustering we merge similar action and separate actions with different attributes. But we don't know how are these actions associated with the language.\\
 \section{Properties of Language :}
 A language has four properties :-
 \paragraph{lexicons : }
  Lexicons of a language are all the words consisting of the language. This includes all words in language and corresponding lexemes. Any sentence in a language is a subset of lexicons. To recognise the lexemes one must be able to understand the word boundaries. A child knows the words boundary very late in it's learning process\\
 \paragraph{Phonology :}
 Phonology of a language corresponds to the pronunciations in the language. Syllables of a language represent the phonological property of the language. In early learning process of child, it doesn't know the word boundaries. So it tries to map syllables to the objects and actions.
 
\paragraph{Morphology :}
The lexemes can be broken as morphemes. Morphemes are the smallest fragmentation of words which have meaning. To able to recognise the lexemes in a language one need to know the meanings of words and be able to distinguish between meaningful and meaningless syllables

\paragraph{Syntax :} 
 Syntax is structure of the sentences in a language. Syntax can be learnt only with prior knowledge of lexemes, phonology and morphology of the language.\\ \\

\hspace*{10pt} Here we study only lexical and phonological properties of the language i.e syllables and words. And we don't assume any prior information about any aspect of the language. So our model will be true for any language.
%% add more text 

\section{Noun Learning :}
 Before discovering the verbs, we need to filter out the nouns in the visual sequence. Therefore we need to discover the nouns in the commentary. These nouns will primarily correspond to the moving objects and the objects more involved into the interactions. To identify the nouns, we need to build attention model :-

 \subsection{Attention Model for objects :}
 When any word in the commentary is said about some object in the video, we say that the object is attended.\\
	The attention model says that the subject will attend to the  objects which are moving. The subject will unlikely utter about the temporarily or permanently static object. he utters about the objects where his gaze is focused. And the gaze of human is more likely to follow the motions in a video. In in a single image, the salient parts are mostly the more contrast parts and the rare or different object. But in image sequences, these factors affect the gaze much less than motion. Therefore the words in the commentary will be highly co-related to the objects which are moving in the video. Based on this assumption we identify the nouns.\\

\subsection{Determining concepts :}

\subsection{Association Measures :}
Association measures are computations on the labels and visual sequences which provide the measure of co-occurrence of given label and visual sequence.\\
Given a label $l$, a concept $c$ at time $t$, following probabilities are defined :-\\
Probability that speaker $s$ has attended the concept $c$ at time $t$ is :-\\

$P(c \| s, t)$= 1 \text{ if c is attended by speaker s at time t} 0 \text{ otherwise}

$P(l \| s, t)$ = 1 \text{ if l is uttered by speaker s at time t} 0 \text{ otherwise}	

Let's assume all the speakers are $S$ and the concepts are represented by $C$ anl the utterances are $L$. \\
Now the joint probability of the concept $c$ and utterance $l$ for all the speakers is represented as :- 
$$J(l,c) = \frac{1}{T*\|S\|} * \sum_{t =1 } ^{T}{\sum_{s\in S}{P(c|s, t) *P(l\|s, t)}}$$
Where : $T$ is total duration of the occurrence of concept \\
In the same way, the concept probability is defined : \\
$$P(c) = \frac{1}{T*\|S\|} * \sum_{t =1 } ^{T}{\sum_{s\in S}{P(c|s, t)}}$$
and label probability is : 
$$P(l) = \frac{f(l)}{\sum_l f(l)}$$
Where $f(l)$ is the frequency of label $l$ in the commentary\\
A good association measure is one which gives very high values for the labels and visual categories which co-occur frequently. It also penalize the labels which occur frequently with different visual categories. Here are some association measures used in in this project\\


\paragraph{Relative Frequency :}
This measure is used in both the noun discovery as well as in verb discovery. For nouns this does not give good results because the nouns are uniformly distributed in the commentary. Relative frequency is calculated as follows :
$$RF(l,c) = \frac{Frequncy of l when c is in focus}{(freq of l) *(freq of l when c is not in focus)}$$
Relative frequency gives high measures of words occurring in relatively high frequency. But it also gives high values for those words which occur seldom in the commentary. These words spoil the noun results with relative frequency measure. The solution for this is mutual information measure. 


\paragraph{Mutual Information : }
Mutual information gives weight to the occurrence of a label with respect to the all words occurring for the visual sequence. Therefore it doesn't give weight to the labels seldom occurring in the commentary. Apart from that it favours, the rare concepts which co-occur with specific label frequently. Mutual information is defined as :- \\
 $$MI(l, c) = J(l,c)*log\left(\frac{J(l,c)}{P(c)*P(l)}\right)$$


\section{Verb Learning }
%\colorbox{gray}{\Large \bf A. Completely Unsupervised Method}\\
\subsection{Completely Unsupervised Method}
Here we get labels for the action clusters generated by hierarchical clustering. Since these clusters have larger distance between them as compared to the distances between their subsets, they are assumed to represent separate action. Therefore each cluster is treated as a separate concept. Now for labels in each concept we calculate the relative frequency measure. After ranking the values in decreasing order, we get that top-most values have labels mostly verbs, rare words and some of nouns. Here we don't get nouns because nouns are uniformly distributed in all the clusters.\\
We filter out top labels based on some threshold. Now we do more filtering by Hindi 	corpus and previously determined nouns. After this process, we majorly get the verbs for each cluster. In a cluster, verbs with highest measure and with synonyms of high measures are the action labels of the the cluster.\\


%\colorbox{gray}{\Large \bf B. User Labelled Unsupervised Method}\\
\subsection{User Labelled Unsupervised Method}
We use similar method for learning the verbs in user labelled action clusters as used for hierarchical cluster based action cluster.
Each of the clustered intervals are treated as separate concepts. Now for labels each concept, we calculate the relative frequency measure. Here again we get most verbs, rare words and some nouns as top words. Then we filter words based on association measure value, corpus and previously calculated nouns. After that we get the verbs as labels for the cluster. We compare this with the user labelled values for accuracy og results.\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results}
\hrule height 2pt
\vspace*{10pt} 

%%%%%%%%%% APPENDIX %%%%%%%%%%
\appendix
\chapter{Hough Transform}

\hrule height 2pt
\vspace*{10pt} 

\hspace*{10pt} Given a point $(x, y)$ on a line $L$. If the distance of line from origin is $r$ and the normal to the line from origin makes $\theta$ angle from the positive $x-axis$ then, the equation of the line can be written as :-\\
$$r = x cos\theta + y sin\theta$$
For each point $(x,y)$ in the combined data of triangle we get the corresponding $(r, \theta)$ pairs for $\theta \in (-180, 180]$ at the intervals of $\theta _ 0$. In this way $(R, \Theta)$ space is created. In this space the local maxima correspond to line segments. Higher the intensity of the maxima, larger the line segment corresponding to the point.\\

\chapter{Hidden Markov Models}

\hrule height 2pt
\vspace*{10pt} 

Hidden Markov Models(HMM) are dynamic Bayesian network. HMM are modelled as certain number of hidden states and some visible state. All of these states have probabilistic dependencies and these probabilistic dependencies are the characteristics of a particular HMM. Mathematically, these are modelled as follows :-\\
$$\lambda = \{A, B, \pi \}$$
Where : 
\hspace*{20pt} $A :$ State Transition Probabilities 
\hspace*{20pt} $B :$ Observation Symbol Probabilities
\hspace*{20pt} $\pi :$ Initial Probability Distribution
Like all dynamic Bayesian networks, the value of a state in HMM depends on values of  previous $k$ states. In this project, we take $k$ as $1$. So here the value of a state depends only on the previous state

\section{Learning an HMM }
We initialise the probabilities of HMM with selective random values i.e $\pi$. Now given the data, these probabilities are learnt by iteration. The values of these probabilities after these iterations are the characteristics of the HMM of the data.

\section{Loglikelihood :}
Given a learnt HMM $\lambda $ on some data $S$, and given a test $S_0$ data we can measure the similarity of this data to the HMM. This value is represented as :- \\
$$logLik = log(P(S_0 | \lambda))$$
This value is used to measure the distances between two HMMs.\\
\section{Distances Between HMMs}
Let a HMM learnt on Data $S_1$ of length $T_1$ is $\lambda _1$ and that on data $S_2$ of length $T_2$ is $\lambda _ 2$. The distance between the HMMs is defined as :- \\

$$dist(S_1, S_2) = \frac{|logP(S_2|\lambda _ 1)|}{T_1} +\frac{|logP(S_1|\lambda _ 2)|}{T_2} $$
  

%%%%%%%%%% BIBLIOGRAPHY %%%%%%%%%%
 \bibliographystyle{plain}
\bibliography{authorName}
\chapter*{Bibliography}
%
\begin{description}

\item Author, I. (Year). \emph{Book Title}, Publisher; Place of publication.

\item Lamport, L. (1986), \emph{\LaTeX: A Document Preparation System}, Addison-Wesley; Reading, MA.

\item Author, I. (Year). `Journal article title', \emph{Journal}, \textbf{Vol}, pp.first--last.

\item Smith, A.D.A.C. and Wand, M.P. (2008). `Streamlined variance calculations for semiparametric
mixed models', \emph{Statistics in Medicine}, \textbf{27}, pp.435--48.

\end{description}

\end{document}